# -*- coding: utf-8 -*-
"""Fake Job Detection Final Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mo6KdHd54tprlaIG4CoxDbkaV0tC3i3f
"""

!pip install pandas numpy scikit-learn tldextract nltk

!pip install pandas numpy scikit-learn imbalanced-learn tldextract requests tqdm joblib

from google.colab import files
uploaded = files.upload()

!mkdir -p ~/.kaggle
!cp "kaggle (1).json" ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list -s "fake job postings"

!kaggle datasets download -d shivamb/real-or-fake-fake-jobposting-prediction
!unzip -o real-or-fake-fake-jobposting-prediction.zip

import pandas as pd
import numpy as np
import tldextract
import requests
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# =============== LOAD DATASET ===================
df = pd.read_csv("fake_job_postings.csv")
df = df[df['description'].notna()]
df['text'] = (df['title'].fillna('') + ' ' + df['description'].fillna('')).str.strip()

# =============== DOMAIN FEATURES ================
def check_website_status(url):
    try:
        if not isinstance(url, str) or not url.strip():
            return 0
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
        r = requests.head(url, timeout=3, allow_redirects=True)
        return int(r.status_code == 200)
    except:
        return 0

def extract_domain_features(row):
    website = str(row.get('company_profile', '')).lower()
    email = str(row.get('company_email', '')).lower()
    company = str(row.get('company', '')).lower().replace(" ", "")

    extracted = tldextract.extract(website)
    domain = extracted.domain
    suffix = extracted.suffix

    free_domains = ['gmail', 'yahoo', 'hotmail', 'outlook', 'aol', 'protonmail']
    is_free_email = any(f"@{d}." in email for d in free_domains)
    has_website = bool(website.strip()) and ("http" in website or "." in website)
    domain_match = company in domain if company else False
    website_active = check_website_status(website) if has_website else 0

    return pd.Series({
        'has_website': int(has_website),
        'free_email': int(is_free_email),
        'domain_match': int(domain_match),
        'website_active': int(website_active),
        'suffix': suffix
    })

tqdm.pandas()
domain_features = df.progress_apply(extract_domain_features, axis=1)
df = pd.concat([df, domain_features], axis=1)

# =============== SPLIT ==========================
X = df[['text', 'has_website', 'free_email', 'domain_match', 'website_active', 'suffix']]
y = df['fraudulent'].astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# =============== MODEL ==========================
text_col = 'text'
num_cols = ['has_website', 'free_email', 'domain_match', 'website_active']
cat_cols = ['suffix']

preprocessor = ColumnTransformer(
    transformers=[
        ('text', TfidfVectorizer(max_features=5000, stop_words='english'), text_col),
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ]
)

model = Pipeline([
    ('preprocess', preprocessor),
    ('clf', RandomForestClassifier(n_estimators=300, class_weight='balanced', max_depth=15, random_state=42))
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("âœ… Model trained successfully")

joblib.dump(model, "fake_job_model_v3.pkl")
print("ðŸ’¾ Model saved as fake_job_model_v3.pkl")

import pandas as pd
df = pd.read_csv("fake_job_postings.csv")
print(df['fraudulent'].value_counts())
df.head()

df = pd.read_csv("fake_job_postings.csv")
df = df[df['description'].notna()]
df['text'] = (df['title'].fillna('') + ' ' + df['description'].fillna('')).str.strip()
df = df[df['text'].str.len() > 30]  # remove ultra-short ones

print(df['fraudulent'].value_counts())

def check_website_status(url):
    try:
        if not isinstance(url, str) or not url.strip():
            return 0
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
        r = requests.head(url, timeout=3, allow_redirects=True)
        return int(r.status_code == 200)
    except:
        return 0

def extract_domain_features(row):
    website = str(row.get('company_profile', '')).lower()
    email = str(row.get('company_email', '')).lower()
    company = str(row.get('company', '')).lower().replace(" ", "")

    extracted = tldextract.extract(website)
    domain = extracted.domain
    suffix = extracted.suffix

    free_domains = ['gmail', 'yahoo', 'hotmail', 'outlook', 'aol', 'protonmail']
    is_free_email = any(f"@{d}." in email for d in free_domains)
    has_website = bool(website.strip()) and ("http" in website or "." in website)
    domain_match = company in domain if company else False
    website_active = check_website_status(website) if has_website else 0

    return pd.Series({
        'has_website': int(has_website),
        'free_email': int(is_free_email),
        'domain_match': int(domain_match),
        'website_active': int(website_active),
        'suffix': suffix
    })

tqdm.pandas()
domain_features = df.progress_apply(extract_domain_features, axis=1)
df = pd.concat([df, domain_features], axis=1)

X = df[['text', 'has_website', 'free_email', 'domain_match', 'website_active', 'suffix']]
y = df['fraudulent'].astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Balance data using SMOTE (only on numeric/text embeddings)
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_text = tfidf.fit_transform(X_train['text'])
X_test_text = tfidf.transform(X_test['text'])

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_text, y_train)

print("Before SMOTE:", y_train.value_counts().to_dict())
print("After SMOTE:", dict(zip(*np.unique(y_res, return_counts=True))))

text_col = 'text'
num_cols = ['has_website', 'free_email', 'domain_match', 'website_active']
cat_cols = ['suffix']

preprocessor = ColumnTransformer(
    transformers=[
        ('text', TfidfVectorizer(max_features=5000, stop_words='english'), text_col),
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ]
)

model = Pipeline([
    ('preprocess', preprocessor),
    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))
])

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

joblib.dump(model, "fake_job_model_v2.pkl")
print("âœ… Model saved as fake_job_model_v2.pkl")

from google.colab import files
files.download("fake_job_model_v3.pkl")